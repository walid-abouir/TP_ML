{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"float:right; max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Scénario d'Apprentissage Statistique](https://github.com/wikistat/Apprentissage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation de données d'enquête en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a> \n",
    "# Prévision du seuil de revenu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résumé\n",
    "Analyse du revenu ou plutôt du dépassement d'un seuil de revenu sur des données issues d'un sondage aux USA. Modélisation et prévision du dépassement d'un seuil de revenu. Comparaison de la pertinence et de l'efficacité de différentes méthodes de modélisation ou apprentissage.\n",
    "## Introduction\n",
    "Des données publiques disponibles sur le site [UCI repository](http://archive.ics.uci.edu/ml/) sont extraites de la base de données issue du recensement réalisé aux Etats Unis en 1994. Ces données son largement utilisées et font référence comme outil de *benchmark* pour comparer les performances de méthodes d’apprentissage ou modélisation statistique. L’objectif est  de prévoir la variable binaire « revenu annuel » (`income`) supérieur ou inférieur à 50k$. Il ne s’agit pas encore de données massives mais, en concaténant les fichiers fournis d'apprentissage et de test, 48841 individus sont décrits par les 14 variables du tableau ci-dessous. La phase préliminaire de préparation et exploration des données est décrite dans un scénario de la [Saison 2: Exploration](https://github.com/wikistat/Exploration). Les codes de préparation des données sont rapidement repris afin de démarrer du même jeu de données.\n",
    "\n",
    "Num| Libellé |\tEnsemble de valeurs\n",
    "-|--|--|--\n",
    "1|`Age`|\treal\n",
    "2|\t`workClass`|\tPrivate, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n",
    "3|\t`fnlwgt`|\treal\n",
    "4|\t`education`|\tBachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool\n",
    "5|\t`educNum`|\tinteger\n",
    "6|\t`mariStat`|\tMarried-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse\n",
    "7|\t`occup`|\tTech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces\n",
    "8|\t`relationship`|\tWife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n",
    "9|\t`origEthn`|\tWhite, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n",
    "10|\t`sex`|\tFemale, Male\n",
    "11|\t`capitalGain`|\treal  \n",
    "12|\t`capitalLoss`|\treal\n",
    "13|\t`hoursWeek`|\treal\n",
    "14|\t`nativCountry`|\tUnited-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands\n",
    "15|\t`income`|\t\tincHigh (>50K), incLow (<=50K)\n",
    "\n",
    "Une première étape permettant de vérifier, sélectionner, recoder les données, a permis de construire un fichier de type `.csv` qui est utilisé dans ce calepin.\n",
    "\n",
    "\n",
    "**Répondre aux questions en s'aidant des résultats des exécutions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Préparation des données\n",
    "### 1.1 Lecture  \n",
    "\n",
    "Lire les données présentes  dans le répertoire courant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Importations \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "adult=pd.read_csv('adultTrainTest.csv')\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Préparation des données\n",
    "\n",
    "Un travail important de nettoyage des données doit être réalisé pour supprimer les données manquantes, éviter des erreurs de codage, regrouper des modalités bien trop nombreuses, éliminer des variables redondantes. Cette étape de préparation et d'exploration est réalisée dans la [saison 2- exploration](https://github.com/wikistat/Exploration). Seuls sont reproduits ci-dessous les codes permmant la rpéapration du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_categorical_data(df, column_name):\n",
    "    cat_columns = pd.Categorical(df[column_name], ordered=False)\n",
    "    return cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"mariStat\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mariStat\n",
    "cat_name_dic = {\" Never-married\": \"Never-Married\", \" Married-AF-spouse\": \"Married\",\n",
    "                \" Married-civ-spouse\": \"Married\", \" Married-spouse-absent\": \"Not-Married\",\n",
    "                \" Separated\": \"Not-Married\", \" Divorced\": \"Not-Married\", \" Widowed\": \"Widowed\"}\n",
    "adult['mariStat'] = adult.mariStat.map(cat_name_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"nativCountry\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nativCountry\n",
    "cat_country = {\" Cambodia\": \"SE-Asia\", \" Canada\": \"British-Commonwealth\", \" China\": \"China\", \" Columbia\": \"South-America\",\n",
    "               \" Cuba\": \"Other\", \" Dominican-Republic\": \"Latin-America\", \" Ecuador\": \"South-America\",\n",
    "               \" El-Salvador\": \"South-America\", \" England\": \"British-Commonwealth\", \" France\": \"Euro_1\",\n",
    "               \" Germany\": \"Euro_1\", \" Greece\": \"Euro_2\", \" Guatemala\": \"Latin-America\", \" Haiti\": \"Latin-America\",\n",
    "               \" Holand-Netherlands\": \"Euro_1\", \" Honduras\": \"Latin-America\", \" Hong\": \"China\", \" Hungary\": \"Euro_2\",\n",
    "               \" India\": \"British-Commonwealth\", \" Iran\": \"Other\", \" Ireland\": \"British-Commonwealth\", \" Italy\": \"Euro_1\",\n",
    "               \" Jamaica\": \"Latin-America\", \" Japan\": \"Other\", \" Laos\": \"SE-Asia\", \" Mexico\": \"Latin-America\",\n",
    "               \" Nicaragua\": \"Latin-America\", \" Outlying-US(Guam-USVI-etc)\": \"Latin-America\", \" Peru\": \"South-America\",\n",
    "               \" Philippines\": \"SE-Asia\", \" Poland\": \"Euro_2\", \" Portugal\": \"Euro_2\", \" Puerto-Rico\": \"Latin-America\",\n",
    "               \" Scotland\": \"British-Commonwealth\", \" South\": \"Euro_2\", \" Taiwan\": \"China\", \" Thailand\": \"SE-Asia\",\n",
    "               \" Trinadad&Tobago\": \"Latin-America\", \" Vietnam\": \"SE-Asia\", \" United-States\": \"United-States\",\n",
    "               \" Yugoslavia\": \"Euro_2\"}\n",
    "adult[\"nativCountry\"] = adult.nativCountry.map(cat_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"education\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# education\n",
    "cat_educ = {\" 10th\": \"Dropout\", \" 11th\": \"Dropout\", \" 12th\": \"Dropout\", \" 1st-4th\": \"Dropout\", \" 5th-6th\": \"Dropout\",\n",
    "            \" 7th-8th\": \"Dropout\", \" 9th\": \"Dropout\", \" Assoc-acdm\": \"Associates\", \" Assoc-voc\": \"Associates\",\n",
    "            \" Bachelors\": \"Bachelors\", \" Doctorate\": \"Doctorate\", \" HS-grad\": \"HS-grad\", \" Masters\": \"Masters\",\n",
    "            \" Preschool\": \"Dropout\", \" Prof-school\": \"Prof-School\", \" Some-college\": \"HS-Graduate\"}\n",
    "adult[\"education\"] = adult.education.map(cat_educ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"workClass\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workClass\n",
    "cat_work = {\" Federal-gov\": \"Federal-Govt\", \" Local-gov\": \"Other-Govt\", \" State-gov\": \"Other-Govt\", \" Private\": \"Private\",\n",
    "            \" Self-emp-inc\": \"Self-Employed\", \" Self-emp-not-inc\": \"Self-Employed\", \" Without-pay\": \"Not-Working\",\n",
    "            \" Never-worked\": \"Not-Working\"}\n",
    "adult[\"workClass\"] = adult.workClass.map(cat_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"occup\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occup\n",
    "cat_occup = {\" Adm-clerical\": \"Admin\", \" Craft-repair\": \"Blue-Collar\", \" Exec-managerial\": \"White-Collar\",\n",
    "             \" Farming-fishing\": \"Blue-Collar\", \" Handlers-cleaners\": \"Blue-Collar\", \" Machine-op-inspct\": \"Blue-Collar\",\n",
    "             \" Other-service\": \"Service\", \" Priv-house-serv\": \"Service\", \" Prof-specialty\": \"Professional\",\n",
    "             \" Protective-serv\": \"Other-occups\", \" Sales\": \"Sales\", \" Tech-support\": \"Other-occups\",\n",
    "             \" Transport-moving\": \"Blue-Collar\"}\n",
    "adult[\"occup\"] = adult.occup.map(cat_occup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"origEthn\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origEthn\n",
    "cat_orig = {\" White\": \"CaucYes\", \" Black\": \"CaucNo\", \" Amer-Indian-Eskimo\": \"CaucNo\", \" Asian-Pac-Islander\": \"CaucNo\",\n",
    "            \" Other\": \"CaucNo\"}\n",
    "adult[\"origEthn\"] = adult.origEthn.map(cat_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"LcapitalGain\"] = np.log(1 + adult[\"capitalGain\"])\n",
    "adult[\"LcapitalLoss\"] = np.log(1 + adult[\"capitalLoss\"])\n",
    "\n",
    "# capital\n",
    "def quantileCapitalGain(capital):\n",
    "    if type(capital) != int:\n",
    "        result = np.nan\n",
    "    elif capital <= 0:\n",
    "        result = \"None\"\n",
    "    elif capital <= np.median(adult[adult[\"capitalGain\"] > 0][\"capitalGain\"]):\n",
    "        result = \"cgLow\"\n",
    "    else:\n",
    "        result = \"cgHigh\"\n",
    "    return result\n",
    "\n",
    "\n",
    "adult[\"capitalGain\"] = list(map(quantileCapitalGain, adult.capitalGain))\n",
    "\n",
    "def quantileCapitalLoss(capital):\n",
    "    if type(capital) != int:\n",
    "        result = np.nan\n",
    "    elif capital <= 0:\n",
    "        result = \"None\"\n",
    "    elif capital <= np.median(adult[adult[\"capitalLoss\"] > 0][\"capitalLoss\"]):\n",
    "        result = \"clLow\"\n",
    "    else:\n",
    "        result = \"clHigh\"\n",
    "    return result\n",
    "\n",
    "\n",
    "adult[\"capitalLoss\"] = list(map(quantileCapitalLoss, adult.capitalLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"ageQ\"] = pd.qcut(adult.age, 5, labels=[\"Ag1\", \"Ag2\", \"Ag3\", \"Ag4\", \"Ag5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"hoursWeekQ\"] = pd.cut(adult.hoursWeek, bins=np.array([0, 39, 41, 100]), labels=[\"HW1\", \"HW2\", \"HW3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_categorical_data_rename(df, column_name, cat_name_dic):\n",
    "    cat_columns = pd.Categorical(df[column_name], ordered=False)\n",
    "    new_categorie = [cat_name_dic[old_name] for old_name in cat_columns.categories]\n",
    "    return cat_columns.rename_categories(new_categorie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"income\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"income\"] = create_categorical_data_rename(adult, \"income\", {\" <=50K\": \"incLow\", \" >50K\": \"incHigh\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"workClass\", \"education\", \"mariStat\", \"occup\", \"relationship\", \"origEthn\", \"sex\", \"capitalGain\", \n",
    "             \"capitalLoss\", \"nativCountry\"]:\n",
    "    adult[name] = create_categorical_data(adult, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = adult[np.logical_not(adult.isnull().any(axis=1))]\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = adult[(adult[\"sex\"] != \"Female\") | (adult[\"relationship\"] != \"Husband\")]\n",
    "adult = adult[(adult[\"sex\"] != \"Male\") | (adult[\"relationship\"] != \"Wife\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Description élémentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire de la distribution de la variable `age`, de celle `income` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"age\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"fnlwgt\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"income\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult[\"relationship\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.plot(kind=\"scatter\",x=\"age\",y=\"educNum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire des liaisons : `age x hoursWeek`, `age x income`, `sex x income` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.plot(kind=\"scatter\",x=\"hoursWeek\",y=\"age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.boxplot(column=\"age\",by=\"income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** La variable `fnlwgt` (Final sampling weight:  Inverse of sampling fraction adjusted for non-response and over or under sampling of particular groups) est assez obscure.  Que dire de sa liaison avec la variable cible? Elle est supprimée par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.boxplot(column=\"fnlwgt\",by=\"income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mosaic plots\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "mosaic(adult,[\"income\",\"sex\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "mosaic(adult,[\"income\",\"origEthn\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques modifications comlémentaires sont apportées de la base. Certaines variables en versions quantitatives et qualitatives comme le nombre d'heures par semaine, l'âge ou le niveau d'éducation sont conservées. Des variables sont supprimées afin de ne garder qu'une seule présence d'une information sensible: genre et origine ethnique.\n",
    "- Suppression de la variable `fnlwgt` qui n'a guère de signification pour cette analyse.\n",
    "- Création d'une variable binaire `Child`: présence ou non d'enfants.\n",
    "- Suppression de la variable `relationship` redondante avec le genre et le statut marital,\n",
    "- Suppression de la variable `nativCountry` redondante avec l'origine ethnique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sort(adult[\"relationship\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_orig = {' Husband':\"ChildNo\",' Not-in-family':\"ChildNo\",' Other-relative':\"ChildNo\",' Own-child':\"ChildYes\",' Unmarried':\"ChildNo\",' Wife':\"ChildNo\"}\n",
    "adult[\"child\"] = adult.relationship.map(cat_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult=adult.drop([\"fnlwgt\",\"nativCountry\",\"relationship\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Description multivariée\n",
    "Une afcm serait bienvenue à ce niveau de l'étude; elle est exécutée dans le calepin en R avec des graphiques plus élégants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Variables indicatrices\n",
    "\n",
    "**Q** Pourquoi l’introduction de *dummy variables*? Pour quelles méthodes cela serait-il aussi nécessaire en R?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultDum=pd.get_dummies(adult[[\"workClass\",\"education\",\"mariStat\",\n",
    "    \"occup\",\"origEthn\",\"sex\",\"capitalGain\",\"capitalLoss\",\"ageQ\",\"hoursWeekQ\",\"child\"]])\n",
    "adultDum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultJoin = adult[[\"age\",\"educNum\",\"hoursWeek\",\"LcapitalGain\",\"LcapitalLoss\",\"income\"]].join(adultDum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Méthodes de modélisation \n",
    "\n",
    "\n",
    "### 2.1  Extraction des échantillons\n",
    "**Q** Quels sont les objectifs des cellules suivantes ? Justifier la nécessité de ce traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ech = np.random.choice(adultJoin.index.values, 20000, replace=False)\n",
    "adultEch=adultJoin.loc[ind_ech]\n",
    "# Variable cible\n",
    "Y=adultEch[\"income\"]\n",
    "# Variables prédictives\n",
    "X=adultEch.drop([\"income\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=5000,random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 [Régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf)\n",
    "**Q** Commenter  les options de la commande  `GridSearchCV`. A quoi sert `param` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "tps0=time.perf_counter()\n",
    "# Optimisation du paramètre de pénalisation\n",
    "# grille de valeurs\n",
    "param=[{\"C\":[0.8,0.9,1,1.1,1.2]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\",solver=\"liblinear\"), param,cv=10,n_jobs=-1)\n",
    "logitOpt=logit.fit(X_train, Y_train)  # GridSearchCV est lui même un estimateur\n",
    "# paramètre optimal\n",
    "logitOpt.best_params_[\"C\"]\n",
    "tps1=(time.perf_counter()-tps0)\n",
    "print(\"Temps logit = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                              1.-logitOpt.best_score_,logitOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erreur sur l'échantillon test\n",
    "1-logitOpt.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prévision\n",
    "y_chap = logitOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** La matrice de confusion n’est pas symétrique. Quelle pourrait en être la raison ?\n",
    "\n",
    "**Q** Quels algorithmes pourraient être exécutés en R pour la régression logistique? Quelles informations complémentaires en tirer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "LogisticRegression(penalty=\"l1\",C=logitOpt.best_params_['C'],\n",
    "                   solver='liblinear').fit(X_train, Y_train).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 [Arbre binaire de discrimination](http://wikistat.fr/pdf/st-m-app-cart.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tps0=time.perf_counter()\n",
    "# Optimisation de la profondeur de l'arbre\n",
    "param=[{\"max_depth\":list(range(2,10))}]\n",
    "tree= GridSearchCV(DecisionTreeClassifier(),param,cv=10,n_jobs=-1)\n",
    "treeOpt=tree.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.perf_counter()-tps0)\n",
    "print(\"Temps arbre = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                             1. - treeOpt.best_score_,treeOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation de l'erreur de prévision\n",
    "1-treeOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "y_chap = treeOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeOpt.best_params_['max_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydotplus\n",
    "treeG=DecisionTreeClassifier(max_depth=treeOpt.best_params_['max_depth'])\n",
    "treeG.fit(X_train,Y_train)\n",
    "dot_data = StringIO() \n",
    "export_graphviz(treeG, out_file=dot_data) \n",
    "graph=pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_png(\"treeOpt.png\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='treeOpt.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelle est l’insuffisance de l’implémentation des arbres de décision dans Scikit-learn  par rapport à celle de rpart de R ? Que dire de l’arbre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 [Perceptron](http://wikistat.fr/pdf/st-m-app-rn.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "# L'algorithme ds réseaux de neurones nécessite éventuellement une normalisation \n",
    "# des variables explicatives avec les commandes ci-dessous\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train.astype(float))  \n",
    "Xnet_train = scaler.transform(X_train.astype(float))  \n",
    "# Meme transformation sur le test\n",
    "Xnet_test = scaler.transform(X_test.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tps0=time.perf_counter()\n",
    "param_grid=[{\"hidden_layer_sizes\":list([(4,),(5,),(6,)])}]\n",
    "nnet= GridSearchCV(MLPClassifier(max_iter=500),param_grid,cv=10,n_jobs=-1)\n",
    "nnetOpt=nnet.fit(Xnet_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.perf_counter()-tps0)\n",
    "print(\"Temps perceptron = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                                    1. - nnetOpt.best_score_,nnetOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelle stratégie d’optimisation est adoptée ? Quelle autre pourrait l’être? Quel réseau pourrait également être pris en compte? Quelles sont les fonctions d’activation des neurones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation de l'erreur de prévision sur le test\n",
    "1-nnetOpt.score(Xnet_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "y_chap = nnetOpt.predict(Xnet_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 [Forêts aléatoires](http://wikistat.fr/pdf/st-m-app-agreg.pdf)\n",
    "**Q** Commenter les choix de tous les paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# définition des paramètres\n",
    "forest = RandomForestClassifier(n_estimators=500, \n",
    "   criterion='gini', max_depth=None,\n",
    "   min_samples_split=2, min_samples_leaf=1, \n",
    "   max_features='auto', max_leaf_nodes=None,\n",
    "   bootstrap=True, oob_score=True)\n",
    "# apprentissage\n",
    "rfFit = forest.fit(X_train,Y_train)\n",
    "print(1-rfFit.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-rfFit.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimisation du paramètre\n",
    "tps0=time.perf_counter()\n",
    "param=[{\"max_features\":list(range(2,10,1))}]\n",
    "rf= GridSearchCV(RandomForestClassifier(n_estimators=100),param,cv=10,n_jobs=-1)\n",
    "rfOpt=rf.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.perf_counter()-tps0)\n",
    "print(\"Temps r forest = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                                    1. - rfOpt.best_score_,rfOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-rfOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prévision\n",
    "y_chap = rfFit.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf= RandomForestClassifier(n_estimators=100,max_features=6)\n",
    "rfFit=rf.fit(X_train, Y_train)\n",
    "# Importance décroissante des variables\n",
    "importances = rfFit.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(20):\n",
    "    print(X_train.columns[indices[f]], importances[indices[f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphe des importances\n",
    "plt.figure()\n",
    "plt.title(\"Importances des variables\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment est obtenu le graphique ? Quelle importance ? Comment interpréter ces résultats ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 [Gradient boosting](http://wikistat.fr/pdf/st-m-app-agreg.pdf)\n",
    "**Q** Pourquoi pas de paramètre `njobs=-1`? \n",
    "\n",
    "**Q** En plus de celui `optimiser`, quels sont les 2 principaux paramètres cet algorithme laissés par défaut ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "tps0=time.perf_counter()\n",
    "param=[{\"n_estimators\":[200, 250, 300]}]\n",
    "gbm= GridSearchCV(GradientBoostingClassifier(),param,cv=10)\n",
    "gbmOpt=gbm.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "tps1=(time.perf_counter()-tps0)\n",
    "print(\"Temps boosting = %f, Meilleur taux = %f, Meilleur paramètre = %s\" % (tps1,\n",
    "                        1. - gbmOpt.best_score_,gbmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-gbmOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "y_chap = gbmOpt.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_chap,Y_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Comparaison des méthodes\n",
    "\n",
    "### 3.1 [Courbes ROC](http://wikistat.fr/pdf/st-m-app-risque.pdf)\n",
    "**Q** En cohérence avec les résultats précédents, quelle est la courbe la plus au dessus des autres? Commenter ce graphique, que signifie AUC ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "listMethod=[[\"GBM\",gbmOpt],[\"RF\",rfOpt],[\"NN\",nnetOpt],[\"Tree\",treeOpt],[\"Logit\",logitOpt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC des précédents modèles optimaux\n",
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].fit(Xnet_train, Y_train).predict_proba(Xnet_test)\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test,probas_[:,1], pos_label=\"incLow\")\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0]),\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [Validation croisée](http://wikistat.fr/pdf/st-m-app-risque.pdf) *Monte Carlo*\n",
    "**Q** Quelle différence entre la validation croisée Monte Carlo et la V-fold cross validation?\n",
    "\n",
    "**Q** Les variables sont « standardisées ». Pourquoi ? Est-ce important et pour quelles méthodes? Commenter les résultats. Quelle méthode choisir? Quel autre algorithme serait-il pertinent de tester? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "tps0=time.perf_counter()\n",
    "check_random_state(11)\n",
    "# définition des estimateurs\n",
    "logit= LogisticRegression(penalty=\"l1\",solver=\"liblinear\")\n",
    "tree = DecisionTreeClassifier()\n",
    "nnet = MLPClassifier(max_iter=400)\n",
    "rf   = RandomForestClassifier(n_estimators=200)\n",
    "gbm  = GradientBoostingClassifier()\n",
    "# Nombre d'itérations\n",
    "B=10 # pour utiliser le programme, mettre plutôt B=10\n",
    "# définition des grilles de paramètres\n",
    "listMethGrid=[[gbm,{\"n_estimators\":[200, 250, 300]}],\n",
    "    [rf,{\"max_features\":list(range(5,10))}],\n",
    "    [nnet,{\"hidden_layer_sizes\":list([(3,),(4,),(5,)])}],\n",
    "    [tree,{\"max_depth\":list(range(5,10))}],\n",
    "    [logit,{\"C\":[0.8,0.9,1,1.1,1.2]}]]\n",
    "# Initialisation à 0 \n",
    "arrayErreur=np.empty((B,5))\n",
    "for i in range(B):   # itérations sur B échantillons test\n",
    "    # extraction apprentissage et test\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=1000)\n",
    "    scaler = StandardScaler()  \n",
    "    scaler.fit(X_train.astype(float))  \n",
    "    Xnet_train = scaler.transform(X_train.astype(float))  \n",
    "    # Meme transformation sur le test\n",
    "    Xnet_test = scaler.transform(X_test.astype(float))\n",
    "    # optimisation de chaque méthode et calcul de l'erreur sur le test\n",
    "    for j,(method, grid_list) in enumerate(listMethGrid):\n",
    "        methodGrid=GridSearchCV(method,grid_list,cv=10,n_jobs=-1).fit(X_train, Y_train)\n",
    "        methodOpt = methodGrid.best_estimator_\n",
    "        methFit=methodOpt.fit(Xnet_train, Y_train)\n",
    "        arrayErreur[i,j]=1-methFit.score(Xnet_test,Y_test)\n",
    "tps1=time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeErreur=pd.DataFrame(arrayErreur,columns=[\"GBM\",\"RF\",\"NN\",\"Tree\",\"Logit\"])\n",
    "print(\"Temps execution :\",(tps1 - tps0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeErreur[[\"GBM\",\"RF\",\"NN\",\"Tree\",\"Logit\"]].boxplot(return_type='dict')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyennes\n",
    "dataframeErreur.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Q** L'algoritme `GBM` n'a pas été complètement optimisé. Une optimisation fine de l'algorithme `XGBoost` permettra-t-elle une meilleure performance que celle de random forest?\n",
    "\n",
    "**Q** Les SVM ne font pas partie de la comparaison à cause de temps rédhibitoires d’exécution. A quoi est-ce dû ? Commenter les temps d’exécution des différentes étapes. \n",
    "\n",
    "**Q** Les temps d'exécution deviennent longs pour un ordinateur de bureau standard. Comment évaluer l'impact de la taille de l'échantillon sur le temps d'exécution et la précision des prévisions? \n",
    "\n",
    "**Q** Est-il réellement nécessaire d'entraîner de type d'algorithme et pour ce type de problème sur de gros jeux de données?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
